# ===================================================================
# ROOT main.tf - AFTER module declarations
# ===================================================================

module "eks_cluster" {
  source = "./modules/eks-cluster"

  cluster_name = "ayman-eks"
  region       = "us-east-1"
}

module "eks_nodes" {
  source = "./modules/eks-node-group"

  cluster_name    = module.eks_cluster.cluster_name
  node_role_arn   = module.eks_cluster.eks_node_role_arn
  subnet_ids      = module.eks_cluster.private_subnets

  min_size        = 2
  max_size        = 4
  desired_size    = 2
  instance_type   = "t3.medium"
}

# ===================================================================
# EBS CSI Driver Addon (Production IRSA Setup)
# ===================================================================

data "aws_caller_identity" "current" {}

# Dedicated IAM Role for CSI Driver (IRSA)
resource "aws_iam_role" "ebs_csi_driver" {
  name = "${module.eks_cluster.cluster_name}-ebs-csi-driver"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Effect = "Allow"
      Principal = {
        Federated = "arn:aws:iam::${data.aws_caller_identity.current.account_id}:oidc-provider/${replace(module.eks_cluster.cluster_oidc_issuer_url, "https://", "")}"
      }
      Action = "sts:AssumeRoleWithWebIdentity"
      Condition = {
        StringEquals = {
          "${replace(module.eks_cluster.cluster_oidc_issuer_url, "https://", "")}:aud" = "sts.amazonaws.com"
          "${replace(module.eks_cluster.cluster_oidc_issuer_url, "https://", "")}:sub" = "system:serviceaccount:kube-system:ebs-csi-controller-sa"
        }
      }
    }]
  })

  tags = {
    Name = "${module.eks_cluster.cluster_name}-ebs-csi-driver"
  }
}

resource "aws_iam_role_policy_attachment" "ebs_csi_driver_policy" {
  policy_arn = "arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy"
  role       = aws_iam_role.ebs_csi_driver.name
}

# EKS Addon Installation
resource "aws_eks_addon" "ebs_csi" {
  cluster_name             = module.eks_cluster.cluster_name
  addon_name               = "aws-ebs-csi-driver"
  service_account_role_arn = aws_iam_role.ebs_csi_driver.arn

  resolve_conflicts_on_create = "OVERWRITE"
  resolve_conflicts_on_update = "OVERWRITE"

  depends_on = [
    module.eks_cluster,
    module.eks_nodes,
    aws_iam_role.ebs_csi_driver,
    aws_iam_role_policy_attachment.ebs_csi_driver_policy
  ]
}

# ===================================================================
# ROOT OUTPUTS (keep your existing outputs)
# ===================================================================

output "cluster_endpoint" {
  value = module.eks_cluster.cluster_endpoint
}



# ... (keep all your other outputs)

# ---- EXPORT EKS CLUSTER OUTPUTS ----

output "cluster_endpoint" {
  value = module.eks_cluster.cluster_endpoint
}

output "cluster_security_group_id" {
  value = module.eks_cluster.cluster_security_group_id
}

output "vpc_id" {
  value = module.eks_cluster.vpc_id
}

output "private_subnets" {
  value = module.eks_cluster.private_subnets
}

output "public_subnet" {
  value = module.eks_cluster.public_subnet
}

output "eks_node_role_arn" {
  value = module.eks_cluster.eks_node_role_arn
}

output "cluster_name" {
  value = module.eks_cluster.cluster_name
}

output "region" {
  value = module.eks_cluster.region
}

# ---- NODE GROUP OUTPUTS ----

output "node_group_name" {
  value = module.eks_nodes.node_group_name
}

output "node_instance_type" {
  value = module.eks_nodes.node_instance_type
}

output "desired_node_count" {
  value = module.eks_nodes.desired_node_count
}



# modules/eks-cluster/iam.tf

# --- EKS Cluster Role (Control Plane) ---
resource "aws_iam_role" "eks_cluster_role" {
  name = "${var.cluster_name}-eks-cluster-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Effect = "Allow"
      Principal = {
        Service = "eks.amazonaws.com"
      }
      Action = "sts:AssumeRole"
    }]
  })
}

resource "aws_iam_role_policy_attachment" "eks_cluster_policy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
  role       = aws_iam_role.eks_cluster_role.name
}

resource "aws_iam_role_policy_attachment" "eks_service_policy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSServicePolicy"
  role       = aws_iam_role.eks_cluster_role.name
}

# --- EKS Node (Worker) Role ---
resource "aws_iam_role" "eks_node_role" {
  name = "${var.cluster_name}-eks-node-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Effect = "Allow"
      Principal = {
        Service = "ec2.amazonaws.com"
      }
      Action = "sts:AssumeRole"
    }]
  })
}

resource "aws_iam_policy" "ebs_permissions" {
  name        = "EKSNodeEBSPermissions"
  description = "Allow EKS nodes to manage EBS volumes"

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "ec2:CreateVolume",
          "ec2:AttachVolume",
          "ec2:DescribeVolumes",
          "ec2:DeleteVolume",
          "ec2:DetachVolume"
        ]
        Resource = "*"
      }
    ]
  })
}

resource "aws_iam_role_policy_attachment" "ebs_permissions_attach" {
  role       = aws_iam_role.eks_node_role.name
  policy_arn = aws_iam_policy.ebs_permissions.arn
}


# Required policies for EKS workers
resource "aws_iam_role_policy_attachment" "eks_worker_policy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
  role       = aws_iam_role.eks_node_role.name
}

resource "aws_iam_role_policy_attachment" "cni_policy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
  role       = aws_iam_role.eks_node_role.name
}

resource "aws_iam_role_policy_attachment" "ecr_policy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
  role       = aws_iam_role.eks_node_role.name
}

# Optional: Add SSM for debugging (recommended in prod)
resource "aws_iam_role_policy_attachment" "ssm_policy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
  role       = aws_iam_role.eks_node_role.name
}



resource "aws_iam_role_policy_attachment" "ebs_csi" {
  role       = aws_iam_role.eks_node_role.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy"
}

# resource "aws_eks_addon" "ebs_csi" {
#   cluster_name = aws_eks_cluster.eks.name
#   addon_name   = "aws-ebs-csi-driver"
# }
# modules/eks-cluster/main.tf

provider "aws" {
  region = var.region
}

# --- VPC ---
resource "aws_vpc" "eks_vpc" {
  cidr_block           = var.vpc_cidr
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name                           = "${var.cluster_name}-vpc"
    "kubernetes.io/cluster/${var.cluster_name}" = "shared"
  }
}

# Get AZs dynamically
data "aws_availability_zones" "available" {
  state = "available"
}

# --- Public Subnet (for NAT Gateway) ---
resource "aws_subnet" "public" {
  vpc_id                  = aws_vpc.eks_vpc.id
  cidr_block              = var.public_subnet_cidr
  availability_zone       = data.aws_availability_zones.available.names[0]
  map_public_ip_on_launch = true

  tags = {
    Name                                    = "${var.cluster_name}-public-subnet"
    "kubernetes.io/cluster/${var.cluster_name}" = "shared"
    "kubernetes.io/role/elb"                = "1"
  }
}

# --- Private Subnets (for EKS nodes) ---
resource "aws_subnet" "private" {
  count             = length(var.eks_private_subnet_cidrs)
  vpc_id            = aws_vpc.eks_vpc.id
  cidr_block        = var.eks_private_subnet_cidrs[count.index]
  availability_zone = data.aws_availability_zones.available.names[count.index + 1] # avoid AZ0 if used above

  tags = {
    Name                                    = "${var.cluster_name}-private-subnet-${count.index}"
    "kubernetes.io/cluster/${var.cluster_name}" = "shared"
    "kubernetes.io/role/internal-elb"       = "1"
  }
}

# --- Internet Gateway ---
resource "aws_internet_gateway" "igw" {
  vpc_id = aws_vpc.eks_vpc.id

  tags = {
    Name = "${var.cluster_name}-igw"
  }
}

# --- Elastic IP for NAT Gateway ---
resource "aws_eip" "nat" {
  domain = "vpc"
  tags = {
    Name = "${var.cluster_name}-nat-eip"
  }
}

# --- NAT Gateway (in public subnet) ---
resource "aws_nat_gateway" "nat" {
  allocation_id = aws_eip.nat.id
  subnet_id     = aws_subnet.public.id

  tags = {
    Name = "${var.cluster_name}-nat"
  }

  depends_on = [aws_internet_gateway.igw]
}

# --- Route Tables ---
resource "aws_route_table" "public" {
  vpc_id = aws_vpc.eks_vpc.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.igw.id
  }

  tags = {
    Name = "${var.cluster_name}-public-rt"
  }
}

resource "aws_route_table" "private" {
  vpc_id = aws_vpc.eks_vpc.id

  route {
    cidr_block     = "0.0.0.0/0"
    nat_gateway_id = aws_nat_gateway.nat.id
  }

  tags = {
    Name = "${var.cluster_name}-private-rt"
  }
}

# --- Route Table Associations ---
resource "aws_route_table_association" "public" {
  subnet_id      = aws_subnet.public.id
  route_table_id = aws_route_table.public.id
}

resource "aws_route_table_association" "private" {
  count          = length(aws_subnet.private)
  subnet_id      = aws_subnet.private[count.index].id
  route_table_id = aws_route_table.private.id
}

# --- Security Group for EKS Control Plane ---
resource "aws_security_group" "eks_cluster_sg" {
  vpc_id = aws_vpc.eks_vpc.id

  description = "EKS cluster security group"

  egress {
    protocol  = "-1"
    from_port = 0
    to_port   = 0
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "${var.cluster_name}-cluster-sg"
  }
}

# --- EKS Cluster ---
resource "aws_eks_cluster" "eks" {
  name     = var.cluster_name
  version  = "1.30" # Use latest stable; adjust as needed
  role_arn = aws_iam_role.eks_cluster_role.arn

  vpc_config {
    subnet_ids              = aws_subnet.private[*].id
    endpoint_private_access = true
    endpoint_public_access  = true
    public_access_cidrs     = ["0.0.0.0/0"] # Optional: restrict later if needed
  }

  # Restrict Kubernetes API server access to private subnets only
  # (Note: AWS doesn't allow limiting API access *by subnet*, but you can limit by CIDR.
  # Since private subnets are internal, we rely on network design + security groups.)
  # For tighter control, consider using a Bastion or VPC endpoints.

  depends_on = [
    aws_iam_role_policy_attachment.eks_cluster_policy,
    aws_iam_role_policy_attachment.eks_service_policy
  ]

  tags = {
    Name = var.cluster_name
  }
}


# modules/eks-cluster/variables.tf

variable "region" {
  description = "AWS region"
  type        = string
  default     = "us-east-1"
}

variable "cluster_name" {
  description = "Name of the EKS cluster"
  type        = string
  default     = "OM-SE-Cluster"
}

variable "vpc_cidr" {
  description = "CIDR block for the VPC"
  type        = string
  default     = "10.0.0.0/16"
}

variable "eks_private_subnet_cidrs" {
  description = "List of CIDR blocks for private subnets (2 required)"
  type        = list(string)
  default     = ["10.0.1.0/24", "10.0.2.0/24"]
}

variable "database_private_subnet_cidrs" {
  description = "List of CIDR blocks for private subnets (2 required)"
  type        = list(string)
  default     = ["10.0.3.0/24" , "10.0.4.0/24"]
}

variable "public_subnet_cidr" {
  description = "CIDR block for the public subnet"
  type        = string
  default     = "10.0.0.0/24"
}# modules/eks-node-group/main.tf

provider "aws" {
  region = var.region
}

resource "aws_eks_node_group" "workers" {
  cluster_name    = var.cluster_name
  node_group_name = "${var.cluster_name}-workers"
  node_role_arn   = var.node_role_arn
  subnet_ids      = var.subnet_ids

  instance_types = [var.instance_type]

  scaling_config {
    min_size     = var.min_size
    max_size     = var.max_size
    desired_size = var.desired_size
  }

  capacity_type = "ON_DEMAND"
  disk_size     = 20

  labels = {
    role = "worker"
  }

  tags = {
    Name = "${var.cluster_name}-node-group"
  }

  # Ensure IAM role policies are attached before launching nodes
  depends_on = [
    aws_iam_role_policy_attachment.eks_worker_policy,
    aws_iam_role_policy_attachment.cni_policy,
    aws_iam_role_policy_attachment.ecr_policy
  ]
}

# Re-declare the required IAM policy attachments (idempotent – safe to re-attach)
# These ensure the role has correct permissions even if this module runs standalone

resource "aws_iam_role_policy_attachment" "eks_worker_policy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
  role       = split("/", var.node_role_arn)[length(split("/", var.node_role_arn)) - 1]
}

resource "aws_iam_role_policy_attachment" "cni_policy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
  role       = split("/", var.node_role_arn)[length(split("/", var.node_role_arn)) - 1]
}

resource "aws_iam_role_policy_attachment" "ecr_policy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
  role       = split("/", var.node_role_arn)[length(split("/", var.node_role_arn)) - 1]
}



resource "aws_ebs_volume" "postgres_volume" {
  availability_zone = "us-east-1a"   # لازم نفس AZ بتاعة NodeGroup
  size              = 5              # الحجم بالجيجا
  type              = "gp3"           # أو gp2 لو عايز

  tags = {
    Name = "postgres-ebs-volume"
  }
}


# modules/eks-node-group/outputs.tf

output "node_group_name" {
  value = aws_eks_node_group.workers.node_group_name
}

output "node_instance_type" {
  value = var.instance_type
}

output "desired_node_count" {
  value = var.desired_size
}


output "postgres_volume_id" {
  value = aws_ebs_volume.postgres_volume.id
}
# modules/eks-node-group/variables.tf

variable "cluster_name" {
  description = "Name of the existing EKS cluster"
  type        = string
}

variable "node_role_arn" {
  description = "ARN of the IAM role for EKS worker nodes"
  type        = string
}

variable "subnet_ids" {
  description = "List of private subnet IDs where nodes will run"
  type        = list(string)
  
}

variable "min_size" {
  description = "Minimum number of nodes"
  type        = number
  default     = 2
}

variable "max_size" {
  description = "Maximum number of nodes"
  type        = number
  default     = 4
}

variable "desired_size" {
  description = "Desired number of nodes"
  type        = number
  default     = 2
}

variable "instance_type" {
  description = "EC2 instance type for nodes"
  type        = string
  default     = "t3.medium"
}

variable "region" {
  description = "AWS region"
  type        = string
  default     = "us-east-1"
}